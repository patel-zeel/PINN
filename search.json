[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Physics Informed Neural Networks",
    "section": "",
    "text": "Source: Wikipedia"
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Physics Informed Neural Networks",
    "section": "Introduction",
    "text": "Introduction\nPhysics informed neural networks (PINNs) are a class of neural networks that are trained to leverage physics laws behind a system in addition to data to learn the system’s behavior. Let’s see a few scenarios where PINNs can be used:\n\nAir quality inference at unknown locations using a few sensors at known locations. We can inform the neural network with the physics of diffusion and advection of pollutants.\nGround water modeling. We can inform the neural network with the physics of total water conservation.\n\nIn a nutshell, wherever we have a system that is governed by a set of partial differential equations (PDEs), we can use PINNs to learn the system’s behavior."
  },
  {
    "objectID": "index.html#comparison-among-pinns-nns-and-pde-solvers",
    "href": "index.html#comparison-among-pinns-nns-and-pde-solvers",
    "title": "Physics Informed Neural Networks",
    "section": "Comparison among PINNs, NNs and PDE Solvers",
    "text": "Comparison among PINNs, NNs and PDE Solvers\nHere are the main differences among PINNs, Neural Networks (NNs) and PDE solvers:\n\n\n\n\n\n\n\n\nNNs\nPDE solvers\nPINNs\n\n\n\n\nLeverage only data\nLeverage only physics\nLeverage both data and physics\n\n\nAccurate but may not be physically consistent\nNot very accurate but physically consistent\nAccurate and physically consistent\n\n\nRequire a lot of data\nRequire a lot of computational resources\nRequire less data and computational resources"
  },
  {
    "objectID": "index.html#create-a-simple-pinn",
    "href": "index.html#create-a-simple-pinn",
    "title": "Physics Informed Neural Networks",
    "section": "Create a simple PINN",
    "text": "Create a simple PINN\nLet’s say we have a system that is governed by the following PDE: \\[\\begin{equation}\n\\frac{\\partial u}{\\partial t} = \\frac{\\partial^2 u}{\\partial x^2}\n\\end{equation}\\] where \\(u\\) is a function of \\(x\\) and \\(t\\).\nWe can create a simple PINN to learn the behavior of \\(u(t, x)\\) as a function of \\(t\\) and \\(x\\).\n\nInputs: \\(t\\) and \\(x\\)\nOutput: \\(u(t, x)\\)\n\nLet’s first import the required libraries:\n\nimport os\n\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"  # specify which GPU(s) to be used\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom time import time\nfrom functools import partial\nimport numpy as np\nimport pandas as pd\n\nimport jax\nimport jax.numpy as jnp\n\nimport optax\nimport jaxopt\n\nimport flax.linen as nn\nimport matplotlib.pyplot as plt\n\nFirst, we generate some dummy data.\n\nx_n = jnp.linspace(0, 1, 100).reshape(-1, 1)\nt_n = jnp.linspace(0, 1, 100).reshape(-1, 1)\ny_n = jax.random.uniform(jax.random.PRNGKey(0), (100, 1))\n\n\nclass SimplePINN(nn.Module):\n    @nn.compact\n    def __call__(self, t, x):\n        x = jnp.concatenate([t, x], axis=-1)\n        x = nn.Dense(8)(x)\n        x = nn.relu(x)\n        x = nn.Dense(1)(x)\n        return x.squeeze()\n\n\nmodel = SimplePINN()\n\nConsider x_n and y_n as training data. We can define the data-driven part of the loss function as follows: \\[\n\\mathcal{L}_{data} = \\frac{1}{N}\\sum_{i=1}^{N} (u_i - \\hat{u}_i)^2\n\\]\n\ndef data_driven_loss_fn(params, t_n, x_n, y_n):\n    pred_y_n = model.apply(params, t_n, x_n)\n    return jnp.mean(jnp.square(y_n - pred_y_n))\n\nNote that we can evaluate this loss function only at the training data points where ground truth is available.\nNow, we define the physics-informed part of the loss function as follows: \\[\n\\mathcal{L}_{physics} = \\frac{1}{N}\\sum_{i=1}^{N} \\left(\\frac{\\partial \\hat{u}_i}{\\partial t} - \\frac{\\partial^2 \\hat{u}_i}{\\partial x^2}\\right)^2\n\\]\n\ndef physics_informed_loss_fn(params, t_c, x_c):\n    u_t = jax.grad(model.apply, argnums=1)(params, t_c, x_c)\n    u_xx = jax.grad(model.apply, argnums=2)(params, t_c, x_c)\n\n    return jnp.mean(jnp.square(u_t - u_xx))\n\nHere, argnums is a keyword argument that specifies the argument number of the function model.apply with respect to which we want to take the gradient.\nNote that we can evaluate this loss function at any arbitrary point in the domain. This is useful because we can use a large number of points to inform the neural network with the physics of the system and thus we can use a small number of data points to train the neural network. We will call these points as collocation points.\nWe can combine the above discussed losses and use any optimizer to learn the weights of the neural network. After that we can predict the value of \\(u(t, x)\\) anywhere in the domain.\nWith this basic background, we can now classify PINNs into two categories:\n\nContinuous time PINNs\nDiscrete time PINNs"
  },
  {
    "objectID": "index.html#continuous-time-pinns",
    "href": "index.html#continuous-time-pinns",
    "title": "Physics Informed Neural Networks",
    "section": "Continuous time PINNs",
    "text": "Continuous time PINNs\nIn this type of modeling, we learn the behavior of a system as a continuous function of time i.e. \\(u(t, x)\\). Let’s see a more realistic example of continuous time PINNs.\n\nExample: 1-D fluid flow with Burgers’ equation\nBurgers’ equation is a nonlinear PDE that is used to model a variety of physical phenomena such as turbulence, shock waves, etc. It is given by: \\[\n\\frac{\\partial u}{\\partial t} + u \\frac{\\partial u}{\\partial x} = \\nu \\frac{\\partial^2 u}{\\partial x^2}\n\\] where \\(u\\) is a function of \\(x\\) and \\(t\\) and \\(\\nu\\) is a constant.\nIn this demo, we will use it to model the behavior of a fluid in a pipe (one dimension). We will use the following boundary condition:\n\nAt both ends of the pipe, the fluid velocity is zero. \\[\nu(t, -1) = u(t, 1) = 0\n\\]\n\nWe will use the following initial condition:\n\nAt \\(t=0\\), the fluid velocity follows a negative Sine function. \\[\nu(0, x) = -\\sin(\\pi x)\n\\]\n\nLet’s model this system using a continuous time PINN with the following configuration:\n\nclass Config:\n    nu = 0.01/np.pi\n    train_size = 100\n    collocation_size = 10000\n    neurons_per_layer = [2] + [20]*8 + [1]\n    lr = 1e-3\n    num_epochs = 10000\n    activation_name = \"tanh\"\n\n\nconfig = Config()\n\nFirst, we will load the ground truth data generated from numerical simulation. We will use this data later to compare the performance of the PINN.\n\nfrom scipy.io import loadmat\n\ndata = loadmat(\"burgers_shock.mat\")\nt_gt = data[\"t\"].reshape(-1, 1)\nx_gt = data[\"x\"].reshape(-1, 1)\nu_gt = data[\"usol\"].T\nprint(t_gt.shape, x_gt.shape, u_gt.shape)\n\n(100, 1) (256, 1) (100, 256)\n\n\nLet’s visualize the ground truth data.\n\nplt.figure(figsize=(8, 3))\nimg = plt.imshow(\n    u_gt.T,\n    aspect=\"auto\",\n    cmap=\"rainbow\",\n    origin=\"lower\",\n    extent=[t_gt.min(), t_gt.max(), x_gt.min(), x_gt.max()],\n)\nplt.xlabel(\"t\")\nplt.ylabel(\"x\")\ncb = plt.colorbar(img)\ncb.set_label(\"u\")\n\n\n\n\nNow, we generate the training data. Half of the training data is generated from the initial condition:\n\nt_ic = jnp.zeros((config.train_size // 2, 1))\n\nx_ic_key = jax.random.PRNGKey(1234)\nx_ic = jax.random.uniform(\n    x_ic_key, (config.train_size // 2, 1), minval=-1, maxval=1\n).reshape(-1, 1)\n\nu_ic = -jnp.sin(jnp.pi * x_ic)\nprint(t_ic.shape, x_ic.shape, u_ic.shape)\n\n(50, 1) (50, 1) (50, 1)\n\n\nThe other half of the training data is generated from the boundary condition:\n\nt_bc_key = jax.random.PRNGKey(5678)\nx_bc = jnp.vstack(\n    [-jnp.ones((config.train_size // 4, 1)), jnp.ones((config.train_size // 4, 1))]\n)\nt_bc = jax.random.uniform(t_bc_key, (config.train_size // 2, 1))\nu_bc = jnp.zeros_like(t_bc)\nprint(t_bc.shape, x_bc.shape, u_bc.shape)\n\n(50, 1) (50, 1) (50, 1)\n\n\nWe can now combine and visualize the training data as follows:\n\nt_n = jnp.vstack([t_ic, t_bc])\nx_n = jnp.vstack([x_ic, x_bc])\nu_n = jnp.vstack([u_ic, u_bc])\n\nplt.figure(figsize=(8, 3))\nplt.scatter(t_n, x_n, c=u_n, cmap=\"viridis\")\ncb = plt.colorbar()\ncb.set_label(\"u\")\nplt.xlabel(\"t\")\nplt.ylabel(\"x\")\n_ = plt.title(\"Training data\")\n\n\n\n\nNow, we generate the collocation points. We will generate them randomly in the domain.\n\nfrom pyDOE2.doe_lhs import lhs\nfrom random import seed\n\nlhs_points = lhs(2, config.collocation_size, random_state=0)\nt_c = lhs_points[:, 0].reshape(-1, 1)\nx_c = lhs_points[:, 1].reshape(-1, 1) * 2 - 1\n\nplt.figure(figsize=(8, 3))\nplt.scatter(t_c, x_c, c=\"k\", s=1)\nplt.xlabel(\"t\")\nplt.ylabel(\"x\")\n_ = plt.title(\"Collocation points (with no ground truth)\")\n\n\n\n\nWe can define the data-driven and physics-informed loss as follows: \\[\n\\mathcal{L}_{data} = \\frac{1}{N}\\sum_{i=1}^{N} \\left(\\hat{u}_i - u_i\\right)^2\n\\]\n\\[\n\\mathcal{L}_{physics} = \\frac{1}{N}\\sum_{i=1}^{N} \\left(\\frac{\\partial \\hat{u}_i}{\\partial t} + \\hat{u}_i \\frac{\\partial \\hat{u}_i}{\\partial x} - \\nu \\frac{\\partial^2 \\hat{u}_i}{\\partial x^2}\\right)^2\n\\]\n\ndef data_driven_loss_fn(params, t_n, x_n, u_n):\n    u_pred = model.apply(params, t_n, x_n)\n    return jnp.mean(jnp.square(u_pred.ravel() - u_n.ravel()))\n\n\ndef physics_informed_loss_fn(params, t_c, x_c):\n    def loss_for_a_single_datapoint(t, x):\n        u = model.apply(params, t, x).squeeze()\n        u_t = jax.grad(model.apply, argnums=1)(params, t, x).squeeze()\n        u_x = jax.grad(model.apply, argnums=2)(params, t, x).squeeze()\n        u_xx = jax.jacrev(jax.jacrev(model.apply, argnums=2), argnums=2)(params, t, x).squeeze()\n        return jnp.square(u_t + u * u_x - config.nu * u_xx)\n\n    return jax.vmap(loss_for_a_single_datapoint)(t_c, x_c).mean()\n\nNow, we define the neural network model as follows:\n\nclass NN(nn.Module):\n    config: Config\n\n    @nn.compact\n    def __call__(self, t, x):\n        t = 2 * t - 1\n        activation = getattr(nn, self.config.activation_name)\n        output = jnp.hstack([t, x])\n        for i in range(len(self.config.neurons_per_layer) - 1):\n            output = nn.Dense(self.config.neurons_per_layer[i])(output)\n            output = activation(output)\n\n        output = nn.Dense(self.config.neurons_per_layer[-1])(output)\n        return output.squeeze()\n\nLet’s define a fit and predict method that we can use for training and inference. These are some jax specific tricks that we need to use to make sure that the code is compiled and runs efficiently.\n\ndef fit(config, params, loss_fn, input_data, output_data):\n    partial_loss_fn = partial(loss_fn, input_data=input_data, output_data=output_data)\n    value_and_grad_fn = jax.jit(jax.value_and_grad(partial_loss_fn))\n\n    optimizer = optax.adam(config.lr)\n    state = optimizer.init(params)\n\n    @jax.jit\n    def step_fn(params_and_state, xs):\n        params, state = params_and_state\n        value, grads = value_and_grad_fn(params)\n        grads, state = optimizer.update(grads, state)\n        params = optax.apply_updates(params, grads)\n        return (params, state), (value, params)\n\n    (params, state), (loss_history, params_history) = jax.lax.scan(\n        step_fn, (params, state), xs=None, length=config.num_epochs\n    )\n    return params, (loss_history, params_history)\n\n\ndef predict(model, params, input_data):\n    return model.apply(params, *input_data)\n\nFirst, we will train the model without any physics-informed loss. We will use the data-driven loss to train the model.\n\nconfig = Config()\n\nmodel = NN(config)\nparams = model.init(jax.random.PRNGKey(0), t_n, x_n)\n\n\ndef loss_fn(params, input_data, output_data):\n    t, x = input_data\n    u = output_data\n    return data_driven_loss_fn(params, t, x, u)\n\n\ninit = time()\nparams, (loss_history, params_history) = fit(config, params, loss_fn, (t_n, x_n), u_n)\nprint(f\"Training time: {time() - init:.2f} seconds\")\n\nplt.figure(figsize=(8, 3))\nplt.plot(loss_history)\nplt.xlabel(\"Epoch\")\n_ = plt.ylabel(\"Loss\")\n\nTraining time: 4.55 seconds\n\n\n\n\n\nLet’s see what the model has learned.\n\ndef plot(model, params):\n    T, X = np.meshgrid(t_gt, x_gt)\n    pred_u = predict(model, params, (T.reshape(-1, 1), X.reshape(-1, 1)))\n\n    fig, axes = plt.subplots(1, 2, figsize=(10, 3), sharey=True, sharex=True)\n\n    for ax, u, title in zip(\n        axes, [u_gt, pred_u.reshape(T.shape).T], [\"Ground truth\", \"Prediction\"]\n    ):\n        img = ax.imshow(\n            u.T,\n            aspect=\"auto\",\n            cmap=\"rainbow\",\n            origin=\"lower\",\n            extent=[t_gt.min(), t_gt.max(), x_gt.min(), x_gt.max()],\n        )\n        ax.set_xlabel(\"t\")\n        ax.set_ylabel(\"x\")\n        ax.set_title(title)\n\n    cb = fig.colorbar(img, ax=axes.ravel().tolist())\n    cb.set_label(\"u\")\n    fig.suptitle(\n        f\"RMSE: {np.sqrt(np.mean((u_gt.ravel() - pred_u.reshape(T.shape).T.ravel())**2)):.4f}\",\n    )\n    # adjust for colorbar in tight layout\n    plt.tight_layout(rect=[0, 0, 0.7, 1])\n\n\nplot(model, params)\n\n\n\n\nWe can see that the prediction is not very good. Let’s now train it by including the physics-informed loss.\n\nparams = model.init(jax.random.PRNGKey(1), t_n, x_n)\n\n\ndef loss_fn(params, input_data, output_data):\n    t_ic, x_ic, t_c, x_c = input_data\n    u = output_data\n    return data_driven_loss_fn(params, t_ic, x_ic, u) + physics_informed_loss_fn(\n        params, t_c, x_c\n    )\n\n\ninit = time()\nparams, (loss_history, params_history) = fit(\n    config, params, loss_fn, (t_ic, x_ic, t_c, x_c), u_ic\n)\n\nprint(f\"Training time: {time() - init:.2f} seconds\")\n\nplt.figure(figsize=(8, 3))\nplt.plot(loss_history)\nplt.xlabel(\"Epoch\")\n_ = plt.ylabel(\"Loss\")\n\nTraining time: 30.85 seconds\n\n\n\n\n\nLet’s see what physics-informed model has learned.\n\nplot(model, params)\n\n\n\n\nWe can also compare the performance at a perticular time step.\n\ntimes = [0.25, 0.5, 0.75]\nfig, axes = plt.subplots(1, 3, figsize=(12, 3))\nT, X = np.meshgrid(t_gt, x_gt)\ndf = pd.DataFrame(np.hstack([T.reshape(-1, 1), X.reshape(-1, 1), u_gt.T.reshape(-1, 1)]), columns=[\"t\", \"x\", \"u\"])\n\nfor ax, timestep in zip(axes, times):\n    idx = df[\"t\"] == timestep\n    t_t = df[\"t\"][idx].values.reshape(-1, 1)\n    x_t = df[\"x\"][idx].values.reshape(-1, 1)\n    u_t = df[\"u\"][idx].values.reshape(-1, 1)\n    pred_u = predict(model, params, (t_t, x_t))\n    ax.plot(x_t, u_t, label=\"Ground truth\")\n    ax.plot(x_t, pred_u, label=\"Prediction\")\n    ax.legend()\nfig.tight_layout();"
  },
  {
    "objectID": "index.html#discrete-time-pinns",
    "href": "index.html#discrete-time-pinns",
    "title": "Physics Informed Neural Networks",
    "section": "Discrete time PINNs",
    "text": "Discrete time PINNs\nWe generally use Runge-Kutta methods to simulate the evolution of a system over time in discrete time-steps. Let’s start with some basics of Runge-Kutta methods.\n\nRunge-Kutta method of order 4 (RK4)\nLet’s say we have a system that is governed by a PDE: \\[\n\\frac{\\partial u}{\\partial t} = f(t, u)\n\\] where \\(u\\) is a function of \\(t\\) and \\(f\\) is a function of \\(u\\) and \\(t\\). This equation says that rate of change in \\(u\\) with respect to time is dependent on \\(u\\) and \\(t\\).\nIf we know the value of \\(u\\) at time \\(t\\), we can use the Runge-Kutta method to find the value of \\(u\\) at a later time \\(t+\\Delta t\\). Here is how we can do it with Runge-Kutta 4th order method: \\[\nu_{t+\\Delta t} = u_t + \\frac{1}{6}(k_1 + 2k_2 + 2k_3 + k_4)\n\\] where\n\\[\\begin{align}\nk_1 &= \\Delta t f(t, u_t) \\\\\nk_2 &= \\Delta t f(t + \\frac{\\Delta t}{2}, u_t + \\Delta t\\frac{k_1}{2}) \\\\\nk_3 &= \\Delta t f(t + \\frac{\\Delta t}{2}, u_t + \\Delta t\\frac{k_2}{2}) \\\\\nk_4 &= \\Delta t f(t + \\Delta t, u_t + \\Delta t k_3)\n\\end{align}\\]\n\n\nGenerealized Runge-Kutta method\nWe can generalize the above equation for \\(q\\) stage Runge-Kutta method as follows: \\[\\begin{align}\nu_{t+c_i\\Delta t} &= u_t + \\Delta t \\sum_{j=1}^{q} a_{ij} k_i\\\\\nu_{t+\\Delta t} &= u_t + \\Delta t \\sum_{j=1}^{q} b_j k_i\\\\\nk_i &= f\\left(u_t + \\Delta t \\sum_{j=1}^{s} a_{ij} k_j, t + c_i \\Delta t\\right)\n\\end{align}\\] where \\(q\\) are number of stages in the Runge-Kutta method and \\(a_{ij}, b_i\\) are the coefficients of the method.\n\n\nExample: 1-D fluid flow with Burgers’ equation\nLet’s continue with the same example of 1-D fluid flow with Burgers’ equation. We will use the same boundary and initial conditions as before. As per our discussion of Runge-Kutta methods, Burgers’ equation can be written as: \\[\n\\frac{\\partial u}{\\partial t} = f(u, t) = -u \\frac{\\partial u}{\\partial x} + \\nu \\frac{\\partial^2 u}{\\partial x^2}\n\\] where \\(u\\) is a function of \\(x\\) and \\(t\\) and \\(\\nu\\) is a constant.\nWe will assume that the values at \\(t=0\\) are known. We will use the Runge-Kutta 100 stage method to find the values at \\(t=0.9\\).\n\n\nInputs and outputs\nIn this problem, neural network takes \\(x\\) as input and outputs intermediate values \\(u_{t+c_i\\Delta t}(x)\\) and the final value \\(u_{t+\\Delta t}(x)\\).\n\nInput: \\(x\\)\nOutput: [\\(u_{t+c_1\\Delta t}(x)\\), \\(u_{t+c_2\\Delta t}(x)\\), …, \\(u_{t+\\Delta t}(x)\\)]\n\n\nNote that we don’t need the values of \\(c_i\\) since they are not needed anywhere in computation.\n\nLet’s load the \\(A\\) and \\(\\mathbf{b}\\) co-efficients of the Runge-Kutta 100 stage method.\n\nq = 100\nRK_weights = np.loadtxt(f\"Butcher_IRK{q}.txt\")\nRK_weights = RK_weights[:q**2+q].reshape(q+1, q)\nRK_weights.shape\n\n(101, 100)\n\n\n\n\nLoss function\nWe can use the RK equation as following to compute the loss function:\n\\[\\begin{align}\nk_i &= -\\hat{u}_{t+c_i\\Delta t}\\frac{\\partial \\hat{u}_{t+c_i\\Delta t}}{\\partial x} + \\nu \\frac{\\partial^2 \\hat{u}_{t+c_i\\Delta t}}{\\partial x^2}, \\quad i=1,2,...,q \\\\\n\\hat{u}_{t+c_i\\Delta t}(x) &= \\hat{u}^{c_i}_t(x) + \\Delta t \\sum_{j=1}^{q} a_{ij} k_j, \\quad i=1,2,...,q \\\\\n\\hat{u}_{t+\\Delta t}(x) &= \\hat{u}^{1}_t(x) + \\Delta t \\sum_{j=1}^{q} b_j k_j \\\\\n\\mathcal{L}_{data} &= \\sum_{i=1}^{N} \\left( \\left(\\hat{u}^{1}_{t}(x_i) - u_{t}(x_i)\\right)^2 + \\sum_{j=1}^{q} \\left(\\hat{u}^{c_i}_{t}(x_i) - u_{t}(x_i)\\right)^2\\right) \\\\\n\\end{align}\\]\nWe need to satisfy the boundary conditions as well. We can do that by adding the following loss function: \\[\n\\mathcal{L}_{boundary} = \\hat{u}_{t+\\Delta t}(-1)^2 + \\hat{u}_{t+\\Delta t}(1)^2 + \\sum_{j=1}^{q} \\left(\\hat{u}_{t+c_i\\Delta t}(-1)^2 + \\hat{u}_{t+c_i\\Delta t}(1)^2\\right)\n\\]\nTotal loss function is given by: \\[\n\\mathcal{L} = \\mathcal{L}_{data} + \\mathcal{L}_{boundary}\n\\]\n\ndef loss_fn(params, input_data, output_data):\n    x_i, x_b = input_data\n    u_i = output_data\n    # RK loss\n    u = model.apply(params, x_i)\n    du_dx_fn = jax.jacrev(model.apply, argnums=1)\n    d2u_dx2_fn  = jax.jacrev(du_dx_fn, argnums=1)\n    du_dx = jax.vmap(du_dx_fn, in_axes=(None, 0))(params, x_i).squeeze()\n    d2u_dx2 = jax.vmap(d2u_dx2_fn, in_axes=(None, 0))(params, x_i).squeeze()\n    f = config.nu * d2u_dx2 - u*du_dx\n    f = f[:, :-1]\n    u0 = u - delta_t * (f@RK_weights.T)\n    l1 = np.mean((u0 - u_i)**2)\n    \n    # Boundary loss\n    u_b = model.apply(params, x_b)\n    l2 = np.mean((u_b.ravel())**2)\n    \n    return l1 + l2\n\n\nclass NN(nn.Module):\n    config: Config\n\n    @nn.compact\n    def __call__(self, x):\n        activation = getattr(nn, self.config.activation_name)\n        output = x\n        for i in range(len(self.config.neurons_per_layer) - 1):\n            output = nn.Dense(self.config.neurons_per_layer[i])(output)\n            output = activation(output)\n\n        output = nn.Dense(self.config.neurons_per_layer[-1])(output)\n        return output.squeeze()\n\n\nq = 100\nt_0 = 0\nt_final = 0.9\n\ndelta_t = t_final - t_0\n\nconfig = Config()\nconfig.neurons_per_layer = [1, 50, 50, 50, q+1]\nconfig.num_epochs = 1000\nconfig.lr = 1e-2\n\n\nmodel = NN(config)\n\n# Initial data\nx_i = df[\"x\"][df[\"t\"]==t_0].values.reshape(-1, 1)\nu_i = df[\"u\"][df[\"t\"]==t_0].values.reshape(-1, 1)\nprint(x_i.shape)\n\n# Boundary data\nx_b = np.array([-1, 1]).reshape(2, 1)\n\nparams = model.init(jax.random.PRNGKey(1), x_i)\n\ninit = time()\nparams, (loss_history, params_history) = fit(config, params, loss_fn, (x_i, x_b), u_i)\nprint(f\"Training time: {time() - init:.2f} seconds\")\n\nplt.plot(loss_history);\n\n(256, 1)\nTraining time: 32.04 seconds\n\n\n\n\n\n\nx_test = df[\"x\"][df[\"t\"]==t_final].values.reshape(-1, 1)\nu_test = df[\"u\"][df[\"t\"]==t_final].values.reshape(-1, 1)\n\npred_u = model.apply(params, x_test)[:, -1]\n\nplt.plot(x_test, u_test, label=\"Ground truth\");\nplt.plot(x_test, pred_u, label=\"Prediction\");\nplt.legend();\n\n\n\n\n\n\nCan we predict over space for a specific time \\(t\\)?\nLet’s try this experiment. We will now forumlate the burgers’ equation as a function of \\(x\\) and \\(t\\) and try to predict the values of \\(u\\) at all times at a different location \\(x+\\Delta x\\):\n\\[\n\\frac{\\partial u}{\\partial x} = f(u, x, t) = \\frac{1}{u}\\left(-\\frac{\\partial u}{\\partial t} + \\nu \\frac{\\partial^2 u}{\\partial x^2}\\right)\n\\]\nFor a specific time \\(t\\), the above equation can be written as: \\[\n\\frac{\\partial u}{\\partial x} = f(u, x) = \\frac{1}{u}\\left(\\nu \\frac{\\partial^2 u}{\\partial x^2}\\right)\n\\]\n\ndef loss_fn(params, input_data, output_data):\n    t_ic, x_ic, x_b = input_data\n    u_ic = output_data\n    \n    # RK loss\n    u = model.apply()"
  },
  {
    "objectID": "playground.html",
    "href": "playground.html",
    "title": ".",
    "section": "",
    "text": "import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n\nimport scipy\nimport numpy as np\n\nfrom nodepy import runge_kutta_method as rk\n\n\nimport numpy as np\nfrom scipy.special import roots_legendre\n\n# Number of stages\ns = 3\n\n# Generate the roots and weights for the Gauss-Legendre quadrature rule\nx, w = roots_legendre(s)\n\n# Generate the A matrix\nA = np.zeros((s, s))\nfor i in range(s):\n    for j in range(s):\n        if j == i:\n            A[i, j] = x[i] * (1 - x[i])\n        else:\n            A[i, j] = (x[i] + x[j]) / ((x[i] - x[j]) * (1 - x[i] * x[j]))\n\n# Generate the B and C vectors\nB = np.array([(w[i] / 2) for i in range(s)])\nC = np.array([((1 + x[i]) / 2) for i in range(s)])\n\n# Normalize the B and C vectors\nB /= np.sum(B)\nC /= np.sum(C)\n\n# Print the Butcher table\nprint(\"A:\")\nprint(A)\nprint(\"B:\")\nprint(B)\nprint(\"C:\")\nprint(C)\n\nA:\n[[-1.37459667  1.         -0.        ]\n [-1.          0.         -1.        ]\n [ 0.          1.          0.17459667]]\nB:\n[0.27777778 0.44444444 0.27777778]\nC:\n[0.07513444 0.33333333 0.59153222]"
  },
  {
    "objectID": "Burgers_Equation.html",
    "href": "Burgers_Equation.html",
    "title": "Physics-informed Neural Networks for Burgers’ Equation",
    "section": "",
    "text": "Inspired from https://maziarraissi.github.io/PINNs/ and corresponding papers * Physics Informed Deep Learning (Part I): Data-driven Solutions of Nonlinear Partial Differential Equations * Physics Informed Deep Learning (Part II): Data-driven Discovery of Nonlinear Partial Differential Equations\nBurgers’ equation is a nonlinear partial differential equation that describes the evolution of a velocity field in a fluid. It is a simplified model of the Navier-Stokes equations, which are used to describe the motion of fluids. Some interesting videos I watched on Navier-Stokes equations are: * The million dollar equation (Navier-Stokes equations) - a video created with manim * Navier-Stokes Equations - Numberphile * Why 5/3 is a fundamental constant for turbulence - A video by 3Blue1Brown * Navier Stokes Equation | A Million-Dollar Question in Fluid Mechanics * Navier-Stokes Equations - 3Blue1Brown\nThe burgers’ equation and its boundary conditions as used by Raissi et al. are:\n\\[\n\\begin{array}{l}\nu(t, x) : \\text{velocity of the fluid at time } t \\text{ and position } x,\\\\\n\\text{support : } x \\in [-1,1],\\ \\ \\ t \\in [0,1],\\\\\n\\text{constraint : }\\frac{\\partial u}{\\partial t} + u \\frac{\\partial u}{\\partial x} - (0.01/\\pi) \\frac{\\partial^2 u}{\\partial x^2} = 0\\\\\n\\text{initial \\& boundary conditions: } u(0,x) = -\\sin(\\pi x), \\ \\ u(t,-1) = u(t,1) = 0\n\\end{array}\n\\]\nHere, we are going to use a neural network to approximate \\(u(t, x)\\) function."
  },
  {
    "objectID": "Burgers_Equation.html#imports",
    "href": "Burgers_Equation.html#imports",
    "title": "Physics-informed Neural Networks for Burgers’ Equation",
    "section": "Imports",
    "text": "Imports\n\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n\nimport numpy as np\nimport scipy\nfrom sklearn.model_selection import train_test_split\n\nimport jax\nimport jax.random as jr\nimport jax.numpy as jnp\n\nimport flax.linen as nn\nimport optax\n\nimport matplotlib.pyplot as plt\n\n\nfrom pyDOE2 import lhs\n\n/home/patel_zeel/miniconda3/envs/jax_gpu/lib/python3.9/site-packages/flax/struct.py:136: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n  jax.tree_util.register_keypaths(data_clz, keypaths)\n/home/patel_zeel/miniconda3/envs/jax_gpu/lib/python3.9/site-packages/flax/struct.py:136: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n  jax.tree_util.register_keypaths(data_clz, keypaths)"
  },
  {
    "objectID": "Burgers_Equation.html#inference",
    "href": "Burgers_Equation.html#inference",
    "title": "Physics-informed Neural Networks for Burgers’ Equation",
    "section": "Inference",
    "text": "Inference\nHere the task is to use the initial and boundary conditions data to train a neural network to approximate the solution of the PDE. We will use the ground truth data given by Raissi et al. to evaulate the accuracy of the neural network. Here are some pointers: * How was the ground truth data generated? * We will use only 100 data points from initial and boundary conditions to calculate data-driven MSE loss * We will use 10000 data points randomly sampled from the domain to calculate the constraint MSE loss. Note that we do not need any ground truth data to calculate this loss.\n\nLoad and visualize Data\n\ndata = scipy.io.loadmat('burgers_shock.mat')\nt = data['t'].flatten()[:,None]\nx = data['x'].flatten()[:,None]\nExact = np.real(data['usol']).T\nX, T = np.meshgrid(x,t)\nprint(f\"{X.shape=} {T.shape=} {Exact.shape=}\")\n\nplt.figure(figsize=(10, 4))\nplt.imshow(Exact.T, aspect='auto', cmap='rainbow', origin='lower', extent=[t.min(), t.max(), x.min(), x.max()]);\nplt.xlabel('t')\nplt.ylabel('x')\nplt.colorbar();\n\n# plt.scatter(T.ravel(), X.ravel(), c=Exact.ravel(), cmap='coolwarm', s=50);\n\nX.shape=(100, 256) T.shape=(100, 256) Exact.shape=(100, 256)\n\n\n\n\n\n\n\nModel\n\nclass Burgers_NN(nn.Module):\n    layers: list\n    \n    @nn.compact\n    def __call__(self, t, x):\n        t = 2*t - 1\n        X = jnp.concatenate([t.reshape(1,1), x.reshape(1,1)], axis=1)\n        \n        for num_neurons in self.layers:\n            X = nn.Dense(num_neurons)(X)\n            X = nn.tanh(X)\n        \n        y = nn.Dense(1)(X)\n        return y.squeeze()\n\nmodel = Burgers_NN(layers=[2, 200, 200, 200, 200])\n\n\n\nGenerate training data\n\nnp.random.seed(0)\ntrain_size = 100\ncollocation_size = 10000\n\n# Boundary data\nxb = 2 * lhs(1, train_size//2) - 1 # x in [-1, 1]\ntb = np.zeros((train_size//2, 1)) # t = 0\nub = -jnp.sin(jnp.pi * xb) # u(0, x) = -sin(pi*x)\n\n# Initial data\nxi = jnp.ones((train_size//2, 1))\nxi = xi.at[:len(xi)//2].set(-1)\nti = lhs(1, train_size//2) # t in [0, 1]\nui = jnp.zeros_like(ti) # u(t, 0) = 0\n\nx_train = jnp.concatenate([xb, xi], axis=0).ravel()\nt_train = jnp.concatenate([tb, ti], axis=0).ravel()\nu_train = jnp.concatenate([ub, ui], axis=0).ravel()\n\n# Collocation points\nxc = 2 * lhs(1, collocation_size).ravel() - 1 # x in [-1, 1]\ntc = lhs(1, collocation_size).ravel() # t in [0, 1]\n\n\nplt.figure(figsize=(10, 4))\nplt.scatter(t_train, x_train, c=u_train, cmap='rainbow', s=50, label='train points');\nplt.colorbar(label='u(t, x)')\nplt.scatter(tc, xc, c='k', s=1, label='collocation points');\nplt.xlabel('t')\nplt.ylabel('x')\nplt.legend(bbox_to_anchor=(1.2, 1), loc='upper left');\n\n\n\n\n\ndef loss_fn(params, constraint=True):\n    nn_fn = lambda t, x: model.apply(params, t, x)\n    vmap_nn_fn = jax.vmap(nn_fn)\n    u_pred = vmap_nn_fn(t_train, x_train)\n\n    print(f\"{u_pred.shape=} {u_train.shape=}\")\n    assert u_pred.shape == u_train.shape\n    mse = jnp.mean((u_pred - u_train) ** 2)\n\n    if constraint is True:\n        du_dx_fn = jax.grad(nn_fn, argnums=1)\n        du_dx = jax.vmap(du_dx_fn)(tc, xc)\n        print(f\"{du_dx.shape=}\")\n\n        du_dt_fn = jax.grad(nn_fn, argnums=0)\n        du_dt = jax.vmap(du_dt_fn)(tc, xc)\n        print(f\"{du_dt.shape=}\")\n\n        d2u_dx2_fn = jax.grad(du_dx_fn, argnums=1)\n        d2u_dx2 = jax.vmap(d2u_dx2_fn)(tc, xc)\n        print(f\"{d2u_dx2.shape=}\")\n\n        u = vmap_nn_fn(tc, xc)\n        f = du_dt + u * du_dx - (0.01/jnp.pi) * d2u_dx2\n\n        mse += jnp.mean(f ** 2)\n\n    return mse\n\n\ndef fit(value_and_grad_fn, init_params, n_iters):\n    opt = optax.adam(1e-3)\n    init_state = opt.init(init_params)\n\n    @jax.jit\n    def one_step(params_and_state, aux):\n        params, state = params_and_state\n\n        loss, grads = value_and_grad_fn(params)\n\n        updates, state = opt.update(grads, state)\n        params = optax.apply_updates(params, updates)\n\n        return (params, state), loss\n\n    params_and_state = (init_params, init_state)\n    (params, state), loss_history = jax.lax.scan(one_step, params_and_state, None, length=n_iters)\n    return params, loss_history\n\n\nloss_fn(init_params, constraint=True)\n\nu_pred.shape=(100,) u_train.shape=(100,)\n\n\nTraceback (most recent call last):\n  File \"_pydevd_bundle/pydevd_cython.pyx\", line 1081, in _pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\n  File \"_pydevd_bundle/pydevd_cython.pyx\", line 297, in _pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\n  File \"/home/patel_zeel/miniconda3/envs/jax_gpu/lib/python3.9/site-packages/debugpy/_vendored/pydevd/pydevd.py\", line 2023, in do_wait_suspend\n    keep_suspended = self._do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n  File \"/home/patel_zeel/miniconda3/envs/jax_gpu/lib/python3.9/site-packages/debugpy/_vendored/pydevd/pydevd.py\", line 2059, in _do_wait_suspend\n    time.sleep(0.01)\nKeyboardInterrupt\n\n\nKeyboardInterrupt: \n\n\n\nkey = jr.PRNGKey(3)\ninit_params = model.init(key, t_train[0], x_train[0])\nn_iters = 5000\n\nprint(\"Training with constraint\")\nvalue_and_grad_fn = jax.value_and_grad(lambda params: loss_fn(params, constraint=True))\nc_params, c_loss_history = fit(value_and_grad_fn, init_params, n_iters=n_iters)\n\nprint(\"Training without constraint\")\nvalue_and_grad_fn = jax.value_and_grad(lambda params: loss_fn(params, constraint=False))\nparams, loss_history = fit(value_and_grad_fn, init_params, n_iters=n_iters)\n\nplt.plot(loss_history, label='loss without constraint')\nplt.plot(c_loss_history, label='loss with constraint');\nplt.legend();\n\nTraining with constraint\nu_pred.shape=(100,) u_train.shape=(100,)\ndu_dx.shape=(10000,)\ndu_dt.shape=(10000,)\nd2u_dx2.shape=(10000,)\nTraining without constraint\nu_pred.shape=(100,) u_train.shape=(100,)\n\n\n\n\n\n\nparams_list = [c_params, params]\nlabels = ['with constraint', 'without constraint']\npred_fn = jax.jit(jax.vmap(model.apply, in_axes=(None, 0, 0)))\n\n\nfig, ax = plt.subplots(2, 2, figsize=(20, 6))\nfor idx, (tmp_params, label) in enumerate(zip(params_list, labels)):\n    u_pred = pred_fn(tmp_params, T.ravel(), X.ravel())\n    u_pred_train = pred_fn(tmp_params, t_train, x_train)\n\n    mappable = ax[0, idx].imshow(u_pred.reshape(100, 256).T, cmap='rainbow', extent=[0, 1, -1, 1], aspect='auto')\n    fig.colorbar(mappable, ax=ax[0, idx])\n    ax[1, idx].scatter(Exact.ravel(), u_pred, s=1, label=label);\n    ax[1, idx].scatter(u_train, u_pred_train, s=1, label=f'{label} (train points)')\n    ax[1, idx].legend()\n    ax[1, idx].set_xlabel('Exact')\n    ax[1, idx].set_ylabel('Predicted')\n    ax[1, idx].set_ylim(-1.2, 1.2)\n    ax[1, idx].set_xlim(-1.2, 1.2)\n\n\n\n\n\n\nSliced prediction\n\nfig, ax = plt.subplots(1, 3, figsize=(20, 5))\nplt.rc('font', size=18)\nfor axes, t_val in zip(ax, [0.25, 0.50, 0.75]):\n    x = X.ravel()[T.ravel()==t_val]\n    t = jnp.ones_like(x) * t_val\n    u = Exact.ravel()[T.ravel()==t_val]\n    for tmp_params, label in zip(params_list, labels):\n        pred_u = pred_fn(tmp_params, t, x)\n        axes.plot(x, pred_u, label=label)\n    axes.plot(x, u, label='Exact', c='k', linestyle='--', linewidth=2)\n    axes.set_title(f't = {t_val}')\n    axes.set_xlabel('$x$')\n    axes.set_ylabel('$u(t, x)$')\naxes.legend(bbox_to_anchor=(1, 1), loc='upper left');\n\n\n\n\n\nfig, ax = plt.subplots(1, 2, figsize=(20, 6))\ntmp_params = params_list[0]\nu_pred = pred_fn(tmp_params, T.ravel(), X.ravel())\nu_pred_train = pred_fn(tmp_params, t_train, x_train)\n\nmappable = ax[0].imshow(u_pred.reshape(100, 256).T, cmap='rainbow', extent=[0, 1, -1, 1], aspect='auto')\nfig.colorbar(mappable, ax=ax[0])\nax[0].set_xlabel('$t$')\nax[0].set_ylabel('$x$')\nax[0].set_title('PINN prediction')\n\nmappable = ax[1].imshow(Exact.reshape(100, 256).T, cmap='rainbow', extent=[0, 1, -1, 1], aspect='auto')\nfig.colorbar(mappable, ax=ax[1])\nax[1].set_xlabel('$t$')\nax[1].set_ylabel('$x$')\nax[1].set_title('Exact solution');\n\n\n\n\n\n\nMetrics\n\nc_pred_u = pred_fn(c_params, T.ravel(), X.ravel())\npred_u = pred_fn(params, T.ravel(), X.ravel())\n\nprint(f\"RMSE (constrained)= {np.sqrt(np.mean((Exact.ravel() - c_pred_u.ravel())**2)):0.4f}\")\nprint(f\"RMSE (unconstrained)= {np.sqrt(np.mean((Exact.ravel() - pred_u.ravel())**2)):0.4f}\")\n\nRMSE (constrained)= 0.0206\nRMSE (unconstrained)= 0.3494\n\n\n\n\nExtrapolate\n\nt_extra = jnp.linspace(0, 2, 100)\nx_extra = jnp.linspace(-1, 1, 256)\nT_extra, X_extra = jnp.meshgrid(t_extra, x_extra)\n\nu_pred = pred_fn(c_params, T_extra.ravel(), X_extra.ravel())\n\nplt.imshow(u_pred.reshape((100, 256), order='F').T, cmap='rainbow', extent=[T_extra.min(), T_extra.max(), X_extra.min(), X_extra.max()], aspect='auto')\nplt.colorbar();"
  }
]