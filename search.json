[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Physics Informed Neural Networks",
    "section": "",
    "text": "Source: Wikipedia"
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Physics Informed Neural Networks",
    "section": "Introduction",
    "text": "Introduction\nPhysics informed neural networks (PINNs) are a class of neural networks that are trained to leverage physics laws behind a system in addition to data to learn the system’s behavior. Let’s see a few scenarios where PINNs can be used:\n\nAir quality inference at unknown locations using a few sensors at known locations. We can inform the neural network with the physics of diffusion and advection of pollutants.\nGround water modeling. We can inform the neural network with the physics of total water conservation.\n\nIn a nutshell, wherever we have a system that is governed by a set of partial differential equations (PDEs), we can use PINNs to learn the system’s behavior."
  },
  {
    "objectID": "index.html#comparison-among-pinns-nns-and-pde-solvers",
    "href": "index.html#comparison-among-pinns-nns-and-pde-solvers",
    "title": "Physics Informed Neural Networks",
    "section": "Comparison among PINNs, NNs and PDE Solvers",
    "text": "Comparison among PINNs, NNs and PDE Solvers\nHere are the main differences among PINNs, Neural Networks (NNs) and PDE solvers:\n\n\n\n\n\n\n\n\nNNs\nPDE solvers\nPINNs\n\n\n\n\nLeverage only data\nLeverage only physics\nLeverage both data and physics\n\n\nAccurate but may not be physically consistent\nNot very accurate but physically consistent\nAccurate and physically consistent\n\n\nRequire a lot of data\nRequire a lot of computational resources\nRequire less data and computational resources"
  },
  {
    "objectID": "index.html#create-a-simple-pinn",
    "href": "index.html#create-a-simple-pinn",
    "title": "Physics Informed Neural Networks",
    "section": "Create a simple PINN",
    "text": "Create a simple PINN\nLet’s say we have a system that is governed by the following PDE: \\[\\begin{equation}\n\\frac{\\partial u}{\\partial t} = \\frac{\\partial^2 u}{\\partial x^2}\n\\end{equation}\\] where \\(u\\) is a function of \\(x\\) and \\(t\\).\nWe can create a simple PINN to learn the behavior of \\(u(t, x)\\) as a function of \\(t\\) and \\(x\\).\n\nInputs: \\(t\\) and \\(x\\)\nOutput: \\(u(t, x)\\)\n\nLet’s first import the required libraries:\n\nfrom functools import partial\nimport numpy as np\n\nimport jax\nimport jax.numpy as jnp\n\nimport optax\nimport flax.linen as nn\nimport matplotlib.pyplot as plt\n\n/home/patel_zeel/miniconda3/envs/jax_gpu/lib/python3.9/site-packages/flax/struct.py:136: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n  jax.tree_util.register_keypaths(data_clz, keypaths)\n/home/patel_zeel/miniconda3/envs/jax_gpu/lib/python3.9/site-packages/flax/struct.py:136: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n  jax.tree_util.register_keypaths(data_clz, keypaths)\n\n\nFirst, we generate some dummy data.\n\nx_n = jnp.linspace(0, 1, 100).reshape(-1, 1)\nt_n = jnp.linspace(0, 1, 100).reshape(-1, 1)\ny_n = jax.random.uniform(jax.random.PRNGKey(0), (100, 1))\n\n\nNow, we define a two layer neural network model as follows:\n\nclass SimplePINN(nn.Module):\n    @nn.compact\n    def __call__(self, t, x):\n        x = jnp.concatenate([t, x], axis=-1)\n        x = nn.Dense(8)(x)\n        x = nn.relu(x)\n        x = nn.Dense(1)(x)\n        return x\n\n\nmodel = SimplePINN()\n\nConsider x_n and y_n as training data. We can define the data-driven part of the loss function as follows: \\[\n\\mathcal{L}_{data} = \\frac{1}{N}\\sum_{i=1}^{N} (u_i - \\hat{u}_i)^2\n\\]\n\ndef data_driven_loss_fn(params, t_n, x_n, y_n):\n    pred_y_n = model.apply(params, t_n, x_n)\n    return jnp.mean(jnp.square(y_n - pred_y_n))\n\nNote that we can evaluate this loss function only at the training data points where ground truth is available.\nNow, we define the physics-informed part of the loss function as follows: \\[\n\\mathcal{L}_{physics} = \\frac{1}{N}\\sum_{i=1}^{N} \\left(\\frac{\\partial \\hat{u}_i}{\\partial t} - \\frac{\\partial^2 \\hat{u}_i}{\\partial x^2}\\right)^2\n\\]\n\ndef physics_informed_loss_fn(params, t_c, x_c):\n    u_t = jax.grad(model.apply, argnums=1)(params, t_c, x_c)\n    u_xx = jax.grad(model.apply, argnums=2)(params, t_c, x_c)\n\n    return jnp.mean(jnp.square(u_t - u_xx))\n\nHere, argnums is a keyword argument that specifies the argument number of the function model.apply with respect to which we want to take the gradient.\nNote that we can evaluate this loss function at any arbitrary point in the domain. This is useful because we can use a large number of points to inform the neural network with the physics of the system and thus we can use a small number of data points to train the neural network. We will call these points as collocation points.\nWe can combine the above discussed losses and use any optimizer to learn the weights of the neural network. After that we can predict the value of \\(u(t, x)\\) anywhere in the domain.\nWith this basic background, we can now classify PINNs into two categories:\n\nContinuous time PINNs\nDiscrete time PINNs"
  },
  {
    "objectID": "index.html#continuous-time-pinns",
    "href": "index.html#continuous-time-pinns",
    "title": "Physics Informed Neural Networks",
    "section": "Continuous time PINNs",
    "text": "Continuous time PINNs\nIn this type of modeling, we learn the behavior of a system as a continuous function of time i.e. \\(u(t, x)\\). Let’s see a more realistic example of continuous time PINNs.\n\nExample: 1-D fluid flow with Burgers’ equation\nBurgers’ equation is a nonlinear PDE that is used to model a variety of physical phenomena such as turbulence, shock waves, etc. It is given by: \\[\n\\frac{\\partial u}{\\partial t} + u \\frac{\\partial u}{\\partial x} = \\nu \\frac{\\partial^2 u}{\\partial x^2}\n\\] where \\(u\\) is a function of \\(x\\) and \\(t\\) and \\(\\nu\\) is a constant.\nIn this demo, we will use it to model the behavior of a fluid in a pipe (one dimension). We will use the following boundary condition:\n\nAt both ends of the pipe, the fluid velocity is zero. \\[\nu(t, -1) = u(t, 1) = 0\n\\]\n\nWe will use the following initial condition:\n\nAt \\(t=0\\), the fluid velocity follows a negative Sine function. \\[\nu(0, x) = -\\sin(\\pi x)\n\\]\n\nLet’s model this system using a continuous time PINN with the following configuration:\n\nclass Config:\n    nu = 0.01\n    train_size = 100\n    collocation_size = 10000\n    neurons_per_layer = [2, 200, 200, 200, 200, 1]\n    lr = 0.01\n    num_epochs = 10000\n    activation_name = \"relu\"\n\n\nconfig = Config()\n\nFirst, we will load the ground truth data generated from numerical simulation. We will use this data later to compare the performance of the PINN.\n\nfrom scipy.io import loadmat\n\ndata = loadmat(\"burgers_shock.mat\")\nt_gt = data[\"t\"].reshape(-1, 1)\nx_gt = data[\"x\"].reshape(-1, 1)\nu_gt = data[\"usol\"].T\nprint(t_gt.shape, x_gt.shape, u_gt.shape)\n\n(100, 1) (256, 1) (100, 256)\n\n\nLet’s visualize the ground truth data.\n\nplt.figure(figsize=(8, 3))\nimg = plt.imshow(\n    u_gt.T,\n    aspect=\"auto\",\n    cmap=\"rainbow\",\n    origin=\"lower\",\n    extent=[t_gt.min(), t_gt.max(), x_gt.min(), x_gt.max()],\n)\nplt.xlabel(\"t\")\nplt.ylabel(\"x\")\ncb = plt.colorbar(img)\ncb.set_label(\"u\")\n\n\n\n\nNow, we generate the training data. Half of the training data is generated from the initial condition:\n\nt_ic = jnp.zeros((config.train_size // 2, 1))\n\nx_ic_key = jax.random.PRNGKey(1234)\nx_ic = jax.random.uniform(\n    x_ic_key, (config.train_size // 2, 1), minval=-1, maxval=1\n).reshape(-1, 1)\n\nu_ic = -jnp.sin(jnp.pi * x_ic)\nprint(t_ic.shape, x_ic.shape, u_ic.shape)\n\n(50, 1) (50, 1) (50, 1)\n\n\nThe other half of the training data is generated from the boundary condition:\n\nt_bc_key = jax.random.PRNGKey(5678)\nx_bc = jnp.vstack(\n    [-jnp.ones((config.train_size // 4, 1)), jnp.ones((config.train_size // 4, 1))]\n)\nt_bc = jax.random.uniform(t_bc_key, (config.train_size // 2, 1))\nu_bc = jnp.zeros_like(t_bc)\nprint(t_bc.shape, x_bc.shape, u_bc.shape)\n\n(50, 1) (50, 1) (50, 1)\n\n\nWe can now combine and visualize the training data as follows:\n\nt_n = jnp.vstack([t_ic, t_bc])\nx_n = jnp.vstack([x_ic, x_bc])\nu_n = jnp.vstack([u_ic, u_bc])\n\nplt.figure(figsize=(8, 3))\nplt.scatter(t_n, x_n, c=u_n, cmap=\"viridis\")\ncb = plt.colorbar()\ncb.set_label(\"u\")\nplt.xlabel(\"t\")\nplt.ylabel(\"x\")\n_ = plt.title(\"Training data\")\n\n\n\n\nNow, we generate the collocation points. We will generate them randomly in the domain.\n\nfrom pyDOE2.doe_lhs import lhs\nfrom random import seed\n\nlhs_points = lhs(2, config.collocation_size, random_state=0)\nt_c = lhs_points[:, 0].reshape(-1, 1)\nx_c = lhs_points[:, 1].reshape(-1, 1) * 2 - 1\n\nplt.figure(figsize=(8, 3))\nplt.scatter(t_c, x_c, c=\"k\", s=1)\nplt.xlabel(\"t\")\nplt.ylabel(\"x\")\n_ = plt.title(\"Collocation points (with no ground truth)\")\n\n\n\n\nWe can define the data-driven and physics-informed loss as follows: \\[\n\\mathcal{L}_{data} = \\frac{1}{N}\\sum_{i=1}^{N} \\left(\\hat{u}_i - u_i\\right)^2\n\\]\n\\[\n\\mathcal{L}_{physics} = \\frac{1}{N}\\sum_{i=1}^{N} \\left(\\frac{\\partial \\hat{u}_i}{\\partial t} + \\hat{u}_i \\frac{\\partial \\hat{u}_i}{\\partial x} - \\nu \\frac{\\partial^2 \\hat{u}_i}{\\partial x^2}\\right)^2\n\\]\n\ndef data_driven_loss_fn(params, t_n, x_n, u_n):\n    u_pred = model.apply(params, t_n, x_n)\n    return jnp.mean(jnp.square(u_pred - u_n))\n\n\ndef physics_informed_loss_fn(params, t_c, x_c):\n    u = model.apply(params, t_c, x_c)\n    u_t = jax.grad(model.apply, argnums=1)(params, t_c, x_c)\n    u_x = jax.grad(model.apply, argnums=2)(params, t_c, x_c)\n    u_xx = jax.grad(jax.grad(model.apply, argnums=2), argnums=2)(params, t_c, x_c)\n\n    return jnp.mean(jnp.square(u_t + u * u_x - config.nu * u_xx))\n\nNow, we define the neural network model as follows:\n\nclass NN(nn.Module):\n    config: Config\n\n    @nn.compact\n    def __call__(self, t, x):\n        activation = (\n            getattr(nn, self.config.activation_name)\n            if self.config.activation_name != \"sin\"\n            else jnp.sin\n        )\n        output = jnp.hstack([t, x])\n        for i in range(len(self.config.neurons_per_layer) - 1):\n            output = nn.Dense(self.config.neurons_per_layer[i])(output)\n            output = activation(output)\n\n        output = nn.Dense(self.config.neurons_per_layer[-1])(output)\n        return output\n\nLet’s define a fit and predict method that we can use for training and inference. These are some jax specific tricks that we need to use to make sure that the code is compiled and runs efficiently.\n\ndef fit(config, params, loss_fn, input_data, output_data):\n    partial_loss_fn = partial(loss_fn, input_data=input_data, output_data=output_data)\n    value_and_grad_fn = jax.value_and_grad(partial_loss_fn)\n\n    optimizer = optax.adam(config.lr)\n    state = optimizer.init(params)\n\n    @jax.jit\n    def step_fn(params_and_state, xs):\n        params, state = params_and_state\n        value, grads = value_and_grad_fn(params)\n        grads, state = optimizer.update(grads, state)\n        params = optax.apply_updates(params, grads)\n        return (params, state), (value, params)\n\n    (params, state), (loss_history, params_history) = jax.lax.scan(\n        step_fn, (params, state), xs=None, length=config.num_epochs\n    )\n    return params, (loss_history, params_history)\n\n\ndef predict(model, params, input_data):\n    return model.apply(params, *input_data)\n\nFirst, we will train the model without any physics-informed loss. We will use the data-driven loss to train the model.\n\nconfig = Config()\nconfig.activation_name = \"sin\"\nconfig.lr = 1e-4\nconfig.num_epochs = 10000\n\n\nmodel = NN(config)\nparams = model.init(jax.random.PRNGKey(0), t_n, x_n)\n\n\ndef loss_fn(params, input_data, output_data):\n    t, x = input_data\n    u = output_data\n    return data_driven_loss_fn(params, t, x, u)\n\n\nparams, (loss_history, params_history) = fit(config, params, loss_fn, (t_n, x_n), u_n)\n\nplt.figure(figsize=(8, 3))\nplt.plot(loss_history)\nplt.xlabel(\"Epoch\")\n_ = plt.ylabel(\"Loss\")\n\n\n\n\nLet’s see what the model has learned.\n\nT, X = np.meshgrid(t_gt, x_gt)\npred_u = predict(model, params, (T.reshape(-1, 1), X.reshape(-1, 1)))\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 3), sharey=True, sharex=True)\n\nfor ax, u, title in zip(\n    axes, [u_gt, pred_u.reshape(T.shape).T], [\"Ground truth\", \"Prediction\"]\n):\n    img = ax.imshow(\n        u.T,\n        aspect=\"auto\",\n        cmap=\"rainbow\",\n        origin=\"lower\",\n        extent=[t_gt.min(), t_gt.max(), x_gt.min(), x_gt.max()],\n    )\n    ax.set_xlabel(\"t\")\n    ax.set_ylabel(\"x\")\n    ax.set_title(title)\n\ncb = fig.colorbar(img, ax=axes.ravel().tolist())\ncb.set_label(\"u\")\n# plt.tight_layout()"
  },
  {
    "objectID": "playground.html",
    "href": "playground.html",
    "title": ".",
    "section": "",
    "text": "import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n\nimport scipy\nimport numpy as np\n\nfrom nodepy import runge_kutta_method as rk\n\n\nimport numpy as np\nfrom scipy.special import roots_legendre\n\n# Number of stages\ns = 3\n\n# Generate the roots and weights for the Gauss-Legendre quadrature rule\nx, w = roots_legendre(s)\n\n# Generate the A matrix\nA = np.zeros((s, s))\nfor i in range(s):\n    for j in range(s):\n        if j == i:\n            A[i, j] = x[i] * (1 - x[i])\n        else:\n            A[i, j] = (x[i] + x[j]) / ((x[i] - x[j]) * (1 - x[i] * x[j]))\n\n# Generate the B and C vectors\nB = np.array([(w[i] / 2) for i in range(s)])\nC = np.array([((1 + x[i]) / 2) for i in range(s)])\n\n# Normalize the B and C vectors\nB /= np.sum(B)\nC /= np.sum(C)\n\n# Print the Butcher table\nprint(\"A:\")\nprint(A)\nprint(\"B:\")\nprint(B)\nprint(\"C:\")\nprint(C)\n\nA:\n[[-1.37459667  1.         -0.        ]\n [-1.          0.         -1.        ]\n [ 0.          1.          0.17459667]]\nB:\n[0.27777778 0.44444444 0.27777778]\nC:\n[0.07513444 0.33333333 0.59153222]"
  },
  {
    "objectID": "Burgers_Equation.html",
    "href": "Burgers_Equation.html",
    "title": "Physics-informed Neural Networks for Burgers’ Equation",
    "section": "",
    "text": "Inspired from https://maziarraissi.github.io/PINNs/ and corresponding papers * Physics Informed Deep Learning (Part I): Data-driven Solutions of Nonlinear Partial Differential Equations * Physics Informed Deep Learning (Part II): Data-driven Discovery of Nonlinear Partial Differential Equations\nBurgers’ equation is a nonlinear partial differential equation that describes the evolution of a velocity field in a fluid. It is a simplified model of the Navier-Stokes equations, which are used to describe the motion of fluids. Some interesting videos I watched on Navier-Stokes equations are: * The million dollar equation (Navier-Stokes equations) - a video created with manim * Navier-Stokes Equations - Numberphile * Why 5/3 is a fundamental constant for turbulence - A video by 3Blue1Brown * Navier Stokes Equation | A Million-Dollar Question in Fluid Mechanics * Navier-Stokes Equations - 3Blue1Brown\nThe burgers’ equation and its boundary conditions as used by Raissi et al. are:\n\\[\n\\begin{array}{l}\nu(t, x) : \\text{velocity of the fluid at time } t \\text{ and position } x,\\\\\n\\text{support : } x \\in [-1,1],\\ \\ \\ t \\in [0,1],\\\\\n\\text{constraint : }\\frac{\\partial u}{\\partial t} + u \\frac{\\partial u}{\\partial x} - (0.01/\\pi) \\frac{\\partial^2 u}{\\partial x^2} = 0\\\\\n\\text{initial \\& boundary conditions: } u(0,x) = -\\sin(\\pi x), \\ \\ u(t,-1) = u(t,1) = 0\n\\end{array}\n\\]\nHere, we are going to use a neural network to approximate \\(u(t, x)\\) function."
  },
  {
    "objectID": "Burgers_Equation.html#imports",
    "href": "Burgers_Equation.html#imports",
    "title": "Physics-informed Neural Networks for Burgers’ Equation",
    "section": "Imports",
    "text": "Imports\n\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n\nimport numpy as np\nimport scipy\nfrom sklearn.model_selection import train_test_split\n\nimport jax\nimport jax.random as jr\nimport jax.numpy as jnp\n\nimport flax.linen as nn\nimport optax\n\nimport matplotlib.pyplot as plt\n\n\nfrom pyDOE2 import lhs"
  },
  {
    "objectID": "Burgers_Equation.html#inference",
    "href": "Burgers_Equation.html#inference",
    "title": "Physics-informed Neural Networks for Burgers’ Equation",
    "section": "Inference",
    "text": "Inference\nHere the task is to use the initial and boundary conditions data to train a neural network to approximate the solution of the PDE. We will use the ground truth data given by Raissi et al. to evaulate the accuracy of the neural network. Here are some pointers: * How was the ground truth data generated? * We will use only 100 data points from initial and boundary conditions to calculate data-driven MSE loss * We will use 10000 data points randomly sampled from the domain to calculate the constraint MSE loss. Note that we do not need any ground truth data to calculate this loss.\n\nLoad and visualize Data\n\ndata = scipy.io.loadmat('burgers_shock.mat')\nt = data['t'].flatten()[:,None]\nx = data['x'].flatten()[:,None]\nExact = np.real(data['usol']).T\nX, T = np.meshgrid(x,t)\nprint(f\"{X.shape=} {T.shape=} {Exact.shape=}\")\n\nplt.figure(figsize=(10, 4))\nplt.imshow(Exact.T, aspect='auto', cmap='rainbow', origin='lower', extent=[t.min(), t.max(), x.min(), x.max()]);\nplt.xlabel('t')\nplt.ylabel('x')\nplt.colorbar();\n\n# plt.scatter(T.ravel(), X.ravel(), c=Exact.ravel(), cmap='coolwarm', s=50);\n\nX.shape=(100, 256) T.shape=(100, 256) Exact.shape=(100, 256)\n\n\n\n\n\n\n\nModel\n\nclass Burgers_NN(nn.Module):\n    layers: list\n    \n    @nn.compact\n    def __call__(self, t, x):\n        t = 2*t - 1\n        X = jnp.concatenate([t.reshape(1,1), x.reshape(1,1)], axis=1)\n        \n        for num_neurons in self.layers:\n            X = nn.Dense(num_neurons)(X)\n            X = nn.tanh(X)\n        \n        y = nn.Dense(1)(X)\n        return y.squeeze()\n\nmodel = Burgers_NN(layers=[2, 200, 200, 200, 200])\n\n\n\nGenerate training data\n\nnp.random.seed(0)\ntrain_size = 100\ncollocation_size = 10000\n\n# Boundary data\nxb = 2 * lhs(1, train_size//2) - 1 # x in [-1, 1]\ntb = np.zeros((train_size//2, 1)) # t = 0\nub = -jnp.sin(jnp.pi * xb) # u(0, x) = -sin(pi*x)\n\n# Initial data\nxi = jnp.ones((train_size//2, 1))\nxi = xi.at[:len(xi)//2].set(-1)\nti = lhs(1, train_size//2) # t in [0, 1]\nui = jnp.zeros_like(ti) # u(t, 0) = 0\n\nx_train = jnp.concatenate([xb, xi], axis=0).ravel()\nt_train = jnp.concatenate([tb, ti], axis=0).ravel()\nu_train = jnp.concatenate([ub, ui], axis=0).ravel()\n\n# Collocation points\nxc = 2 * lhs(1, collocation_size).ravel() - 1 # x in [-1, 1]\ntc = lhs(1, collocation_size).ravel() # t in [0, 1]\n\n\nplt.figure(figsize=(10, 4))\nplt.scatter(t_train, x_train, c=u_train, cmap='rainbow', s=50, label='train points');\nplt.colorbar(label='u(t, x)')\nplt.scatter(tc, xc, c='k', s=1, label='collocation points');\nplt.xlabel('t')\nplt.ylabel('x')\nplt.legend(bbox_to_anchor=(1.2, 1), loc='upper left');\n\n\n\n\n\ndef loss_fn(params, constraint=True):\n    nn_fn = lambda t, x: model.apply(params, t, x)\n    vmap_nn_fn = jax.vmap(nn_fn)\n    u_pred = vmap_nn_fn(t_train, x_train)\n\n    print(f\"{u_pred.shape=} {u_train.shape=}\")\n    assert u_pred.shape == u_train.shape\n    mse = jnp.mean((u_pred - u_train) ** 2)\n\n    if constraint is True:\n        du_dx_fn = jax.grad(nn_fn, argnums=1)\n        du_dx = jax.vmap(du_dx_fn)(tc, xc)\n        print(f\"{du_dx.shape=}\")\n\n        du_dt_fn = jax.grad(nn_fn, argnums=0)\n        du_dt = jax.vmap(du_dt_fn)(tc, xc)\n        print(f\"{du_dt.shape=}\")\n\n        d2u_dx2_fn = jax.grad(du_dx_fn, argnums=1)\n        d2u_dx2 = jax.vmap(d2u_dx2_fn)(tc, xc)\n        print(f\"{d2u_dx2.shape=}\")\n\n        u = vmap_nn_fn(tc, xc)\n        f = du_dt + u * du_dx - (0.01/jnp.pi) * d2u_dx2\n\n        mse += jnp.mean(f ** 2)\n\n    return mse\n\n\ndef fit(value_and_grad_fn, init_params, n_iters):\n    opt = optax.adam(1e-3)\n    init_state = opt.init(init_params)\n\n    @jax.jit\n    def one_step(params_and_state, aux):\n        params, state = params_and_state\n\n        loss, grads = value_and_grad_fn(params)\n\n        updates, state = opt.update(grads, state)\n        params = optax.apply_updates(params, updates)\n\n        return (params, state), loss\n\n    params_and_state = (init_params, init_state)\n    (params, state), loss_history = jax.lax.scan(one_step, params_and_state, None, length=n_iters)\n    return params, loss_history\n\n\nkey = jr.PRNGKey(3)\ninit_params = model.init(key, t_train[0], x_train[0])\nn_iters = 5000\n\nprint(\"Training with constraint\")\nvalue_and_grad_fn = jax.value_and_grad(lambda params: loss_fn(params, constraint=True))\nc_params, c_loss_history = fit(value_and_grad_fn, init_params, n_iters=n_iters)\n\nprint(\"Training without constraint\")\nvalue_and_grad_fn = jax.value_and_grad(lambda params: loss_fn(params, constraint=False))\nparams, loss_history = fit(value_and_grad_fn, init_params, n_iters=n_iters)\n\nplt.plot(loss_history, label='loss without constraint')\nplt.plot(c_loss_history, label='loss with constraint');\nplt.legend();\n\nTraining with constraint\nu_pred.shape=(100,) u_train.shape=(100,)\ndu_dx.shape=(10000,)\ndu_dt.shape=(10000,)\nd2u_dx2.shape=(10000,)\nTraining without constraint\nu_pred.shape=(100,) u_train.shape=(100,)\n\n\n\n\n\n\nparams_list = [c_params, params]\nlabels = ['with constraint', 'without constraint']\npred_fn = jax.jit(jax.vmap(model.apply, in_axes=(None, 0, 0)))\n\n\nfig, ax = plt.subplots(2, 2, figsize=(20, 6))\nfor idx, (tmp_params, label) in enumerate(zip(params_list, labels)):\n    u_pred = pred_fn(tmp_params, T.ravel(), X.ravel())\n    u_pred_train = pred_fn(tmp_params, t_train, x_train)\n\n    mappable = ax[0, idx].imshow(u_pred.reshape(100, 256).T, cmap='rainbow', extent=[0, 1, -1, 1], aspect='auto')\n    fig.colorbar(mappable, ax=ax[0, idx])\n    ax[1, idx].scatter(Exact.ravel(), u_pred, s=1, label=label);\n    ax[1, idx].scatter(u_train, u_pred_train, s=1, label=f'{label} (train points)')\n    ax[1, idx].legend()\n    ax[1, idx].set_xlabel('Exact')\n    ax[1, idx].set_ylabel('Predicted')\n    ax[1, idx].set_ylim(-1.2, 1.2)\n    ax[1, idx].set_xlim(-1.2, 1.2)\n\n\n\n\n\n\nSliced prediction\n\nfig, ax = plt.subplots(1, 3, figsize=(20, 5))\nplt.rc('font', size=18)\nfor axes, t_val in zip(ax, [0.25, 0.50, 0.75]):\n    x = X.ravel()[T.ravel()==t_val]\n    t = jnp.ones_like(x) * t_val\n    u = Exact.ravel()[T.ravel()==t_val]\n    for tmp_params, label in zip(params_list, labels):\n        pred_u = pred_fn(tmp_params, t, x)\n        axes.plot(x, pred_u, label=label)\n    axes.plot(x, u, label='Exact', c='k', linestyle='--', linewidth=2)\n    axes.set_title(f't = {t_val}')\n    axes.set_xlabel('$x$')\n    axes.set_ylabel('$u(t, x)$')\naxes.legend(bbox_to_anchor=(1, 1), loc='upper left');\n\n\n\n\n\nfig, ax = plt.subplots(1, 2, figsize=(20, 6))\ntmp_params = params_list[0]\nu_pred = pred_fn(tmp_params, T.ravel(), X.ravel())\nu_pred_train = pred_fn(tmp_params, t_train, x_train)\n\nmappable = ax[0].imshow(u_pred.reshape(100, 256).T, cmap='rainbow', extent=[0, 1, -1, 1], aspect='auto')\nfig.colorbar(mappable, ax=ax[0])\nax[0].set_xlabel('$t$')\nax[0].set_ylabel('$x$')\nax[0].set_title('PINN prediction')\n\nmappable = ax[1].imshow(Exact.reshape(100, 256).T, cmap='rainbow', extent=[0, 1, -1, 1], aspect='auto')\nfig.colorbar(mappable, ax=ax[1])\nax[1].set_xlabel('$t$')\nax[1].set_ylabel('$x$')\nax[1].set_title('Exact solution');\n\n\n\n\n\n\nMetrics\n\nc_pred_u = pred_fn(c_params, T.ravel(), X.ravel())\npred_u = pred_fn(params, T.ravel(), X.ravel())\n\nprint(f\"RMSE (constrained)= {np.sqrt(np.mean((Exact.ravel() - c_pred_u.ravel())**2)):0.4f}\")\nprint(f\"RMSE (unconstrained)= {np.sqrt(np.mean((Exact.ravel() - pred_u.ravel())**2)):0.4f}\")\n\nRMSE (constrained)= 0.0206\nRMSE (unconstrained)= 0.3494\n\n\n\n\nExtrapolate\n\nt_extra = jnp.linspace(0, 2, 100)\nx_extra = jnp.linspace(-1, 1, 256)\nT_extra, X_extra = jnp.meshgrid(t_extra, x_extra)\n\nu_pred = pred_fn(c_params, T_extra.ravel(), X_extra.ravel())\n\nplt.imshow(u_pred.reshape((100, 256), order='F').T, cmap='rainbow', extent=[T_extra.min(), T_extra.max(), X_extra.min(), X_extra.max()], aspect='auto')\nplt.colorbar();"
  }
]